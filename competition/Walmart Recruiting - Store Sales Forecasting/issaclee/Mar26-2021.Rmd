---
title: "월마트 대회 with Tidymodels"
output:
  html_document:
    number_sections: true
    fig_caption: true
    toc: true
    fig_width: 5
    fig_height: 4
    theme: cosmo
    highlight: tango
    code_folding: show
editor_options: 
  chunk_output_type: inline
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE,
                      message = FALSE,
                      warning = FALSE,
                      fig.align = "center")
```


능선회귀 (Ridge regression) 튜닝에 대하여 알아보자.

![Photo steal from [here](https://connectedremag.com/das-in-building-wireless/walmart-verizon-explore-testing-5g-in-some-stores/)](https://connectedremag.com/wp-content/uploads/2020/03/walmart-5G-connected-real-estate.png)

본 포스팅은 [슬기로운 통계생활 캐글 R 스터디](https://www.youtube.com/playlist?list=PLKtLBdGREmMlJCXjCpCi5B4KQ-TsFvAAi) 발표용 포스팅입니다.

# 준비작업 {.tabset .tabset-fade}

## 사용 Library 로드

이번 포스팅에서는 핫하디 핫한 `tidymodels` 사용하여 월마트 대회를 가지고 놀아본다. 또한 마이 뻬이보릿 연산자들을 사용하기 위하여 `magrittr`를 불러왔다.

```{r load_lib, message=FALSE, warning=FALSE, results='hide'}
library(tidymodels)
library(tidyverse)
library(magrittr)
library(skimr)
library(knitr)
theme_set(theme_bw())
```

## 데이터 불러오기

```{r}
file_path <- "../input/walmart-recruiting-store-sales-forecasting/"
files <- list.files(file_path)
files
```

```{r, message=FALSE}
train <- read_csv(file.path(file_path, "train.csv.zip")) %>% 
  janitor::clean_names()
test <- read_csv(file.path(file_path, "test.csv.zip")) %>% 
  janitor::clean_names()
features <- read_csv(file.path(file_path, "features.csv.zip")) %>% 
  janitor::clean_names()
stores <- read_csv(file.path(file_path, "stores.csv")) %>% 
  janitor::clean_names()
```

# 데이터 기본정보 확인{.tabset .tabset-fade}

## 기본 데이터

이 대회는 기본적으로 간단한 대회이다. 첫번째 스터디용 대회로 선택을 한 이유이기도 하다. 주 데이터는 42만개의 train 샘플과 11만개의 test 샘플로 구성이 되어있다.

```{r}
dim(train)
dim(test)
```

변수명을 살펴보면, 월마트 가맹점을 뜻하는 `store` 변수와 매장안의 부서들을 나타내는 `dept`, 날짜 정보를 가지고 있는 `date`와 `is_holiday`, 마지막으로 우리의 target 변수인 `weekly_sales`가 있는 것을 확인 할 수 있다.

```{r}
names(train)
names(test)
```

## `train` 데이터 훑어보기

```{r}
skim(train)
```

## `test` 데이터 훑어보기

```{r}
skim(test)
```

## `store` 데이터

`store` 데이터는 상대적으로 간단하다. 각 점포에 대한 사이즈와 타입변수가 담겨져 있다. 타입변수는 월마트에서 운영하는 supercenter와 같이 매장의 성격을 나타내는 변수이다.

```{r}
dim(stores)
head(stores)
```


```{r}
skim(stores)
```

## `feature` 데이터

`feature` 데이터는 조금 복잡한데, 각 점포별로 각 주마다의 정보가 담겨있는 것을 알 수 있다.

```{r}
dim(features)
length(unique(features$Store)) * length(unique(features$Date))

head(features)
```

일단 `NA`의 존재가 많음. `skim()` 함수의 complete 정보를 통하여 알아 볼 수 있다. 또한, 대회 데이터에 대한 설명을 보면 `mark_down1-5` 변수의 경우 월마트에서 진행하고 있는 Promotion을 의미한다. 하지만 이 변수의 경우 2011년 11월 이후에 날짜에 대하여만 접근 가능하고, 그 이전의 경우에는 `NA`로 채워져있다. 이러한 `NA`를 어떻게 사용할 것인가가 이 대회의 핵심일 것 같다.

```{r}
skim(features)
```


# 탐색적 데이터 분석 및 시각화 {.tabset .tabset-fade}

## `weekly_sales`

먼저 우리의 예측 목표인 주간 세일변수 `weekly_sales`를 시각화 해보도록 하자.

```{r message=FALSE, class.source = 'fold-hide'}
train %>% 
  ggplot(aes(x = weekly_sales)) +
  geom_histogram()
```
매출액의 분포라서 오른쪽으로 엄청 치우쳐있는 것을 알 수 있다. 이런 경우 보통 `log` 함수를 취해줘서 분포의 치우침을 잡아준다. 이렇게 분포 치우침을 잡아주는 이유는 회귀분석 같은 전통적인 기법의 경우 데이터에 섞여있는 잡음의 분포를 정규분포같이 대칭인 분포로 가정하는 경우가 많기 때문이다. 

```{r message=FALSE, class.source = 'fold-hide'}
train %>% 
    ggplot(aes(x = sign(weekly_sales) * log(abs(weekly_sales) + 2))) +
    geom_histogram() +
    labs(title = "Transformed distribution of weekly sales 1",
         x = "weekly_sales")
```

`log`를 취해주었을 경우 다음과 같이 치우침이 많이 잡히는 것을 알 수 있다. 하지만 위의 분포 역시 왼쪽으로 치우쳐 있는 것을 알 수 있다. 분포를 조금 더 종모양으로 만들어주기 위하여 제곱근을 이용했다. 아래를 보면 분포가 종모양처럼 예뻐진 것을 알 수 있다.

```{r message=FALSE, class.source = 'fold-hide'}
train %>% 
    ggplot(aes(x = sign(weekly_sales) * (abs(weekly_sales))^(1/5))) +
    geom_histogram() +
    labs(title = "Transformed distribution of weekly sales 2",
         x = "weekly_sales")
```

## 결측치 분석

R에는 결측치 분석을 아주 용이하게 해주는 패키지가 하나 존재하는데, 바로 `naniar`라는 패키지 이다.

```{r message=FALSE, warning=FALSE, class.source = 'fold-hide'}
library(naniar)
features %>% 
  select_if(~sum(is.na(.)) > 0) %>% # 결측치 있는 칼럼 선택
  gg_miss_var()
```
`gg_miss_var()`를 통하여 현재 mark_down1-5 변수, 그리고, unemploment와 cpi가 결측치가 존재하는 것을 확인하였다.

```{r message=FALSE, class.source = 'fold-hide'}
features %>% 
  select_if(~sum(is.na(.)) > 0) %>%
  gg_miss_upset()
```

`gg_miss_upset()` 함수의 경우 결측치가 동시에 발생하는 변수들을 보여주는데, mark_down1-5까지가 동시에 없는 결측치의 경우 (첫번째 기둥) 2011년 11월 이전의 자료를 나타내는 것을 알 수 있다.

# 전처리 레시피(`recipe`) 만들기

tidymodels의 전처리 패키지지 recipe을 사용하여 전처리를 하도록하자.

## `all_data` 합치기

먼저 `store` 와 `features` 데이터에 있는 정보를 `train`과 `test` 데이터에 옮겨오자. 일단 결측치가 없는 변수들만 가져오고, 추후에 결측치가 있는 변수인 cpi와 unemployment, mark_down 변수들은 제외한다.

```{r}
# train weekly_sales 변경
train %<>%
  mutate(weekly_sales = sign(weekly_sales) * (abs(weekly_sales))^(1/5))

all_data <- bind_rows(train, test)
all_data <- left_join(all_data, stores, by = c("store"= "store"))
all_data <- features %>% 
    select(-c(starts_with("mark"), is_holiday, cpi, unemployment)) %>% 
    left_join(all_data, y = ., by = c("store"= "store",
                                      "date" = "date"))

names(all_data)
dim(all_data)
```

테스트 데이터에 들어있는 `weekly_sales` 변수를 제외한 모든 변수에 결측치가 없음을 확인하자.

```{r}
all_data %>% 
    summarise_all(~sum(is.na(.)))
```
## 날짜 데이터 베이스 만들기

미국의 휴일 정보를 가지고있는 step_holiday 함수를 이용해서 미국 공휴일을 모두 빼오도록 한다. 다음은 미국 공휴일 목록이다.

```{r}
timeDate::listHolidays("US")
```

```{r}
library(lubridate)

datedb <- data.frame(date = ymd("2010-1-1") + days(0:(365*4))) %>% 
    filter(date > "2010-01-29" & date < "2013-07-27") %>% 
    mutate(index = 0:(length(date)-1))
datedb$date %>% range()
all_data$date %>% range()

holiday_rec <- recipe(~ date + index, datedb) %>% 
    step_holiday(date,
                 holidays = timeDate::listHolidays("US")) %>% 
    step_mutate(index_mod = index %/% 7) %>% 
    prep(training = datedb) %>% 
    juice()

holiday_rec %<>%
    select(-date) %>% 
    select(starts_with("date"), index_mod) %>% 
    group_by(index_mod) %>% 
    summarise_all(sum) %>% 
    mutate(date = all_data$date %>% unique()) %>% 
    select(date, dplyr::everything())

all_data <- holiday_rec %>% 
    select(-index_mod) %>% 
    left_join(all_data, y = ., by = c("date" = "date"))
all_data %>% head()
```

## 전처리 과정 기록하기

```{r}
walmart_recipe <- all_data %>% 
    recipe(weekly_sales ~ .) %>%
    step_date(date) %>%
    step_rm(date, date_dow) %>%
    step_mutate(store = as_factor(store),
                dept = as_factor(dept)) %>%
    step_dummy(all_nominal()) %>% 
    step_normalize(all_numeric(), -all_outcomes()) %>% 
    prep(training = all_data)

print(walmart_recipe)
```

## 전처리 데이터 짜내기 (`juice`)

저장된 `recipe`의 전처리를 한 데이터를 `juice` 함수로 짜내보자. 

```{r}
all_data2 <- juice(walmart_recipe)
all_data2 %>% head() %>% dim()
```

# 모델 학습하기

## 데이터 나누기

```{r}
train_index <- seq_len(nrow(train))
train2 <- all_data2[train_index,]
test2 <- all_data2[-train_index,]

# 튜닝을 위한 validation 데이터 설정
set.seed(2021)
validation_split <- vfold_cv(train2,  v = 10,
                             strata = weekly_sales)
```

## 튜닝 스펙 설정하기

```{r}
tune_spec <- linear_reg(penalty = tune(), mixture = 0) %>%
  set_engine("glmnet")

lambda_grid <- grid_regular(penalty(), levels = 100)
```

## 워크플로우 `workflow()` 설정

```{r}
workflow <- workflow() %>%
  add_model(tune_spec) %>% 
  add_formula(weekly_sales ~ .)
```

## $\lambda$ 튜닝하기

```{r}
doParallel::registerDoParallel()

tune_result <- workflow %>% 
  tune_grid(validation_split,
            grid = lambda_grid,
            metrics = metric_set(rmse, mae))
```

```{r}
tune_result %>% 
  collect_metrics()
```

## 튜닝 결과 시각화

```{r message=FALSE}
tune_result %>%
  collect_metrics() %>%
  ggplot(aes(penalty, mean, color = .metric)) +
  geom_line(size = 1.5) +
  scale_x_log10() +
  facet_wrap(vars(.metric)) +
  theme(legend.position = "none") +
  labs(title = "RMSE")
```

```{r}
tune_result %>% show_best(metric = "mae")
```

```{r}
tune_best <- tune_result %>% select_best(metric = "mae")
tune_best$penalty
```

# Set Ridge regression model and fitting (Ridge regeression 모델 설정 및 학습)

Set `mixture` is equal to zero refering the Ridge regression in `glmnet` since the 

```{r message=FALSE, warning=FALSE}
ridge_model <- 
    linear_reg(penalty = tune_best$penalty, # tuned penalty
               mixture = 0) %>% # lasso: 1, ridge: 0
    set_engine("glmnet")

ridge_fit <- 
    ridge_model %>% 
    fit(weekly_sales ~ ., data = train2)

options(max.print = 10)
ridge_fit %>% 
    tidy() %>% 
    filter(estimate > 0.001)
```

# Prediction and submit (예측 및 평가)

```{r warning=FALSE}
result <- predict(ridge_fit, test2)
result %>% head()
result %<>%
  mutate(.pred = sign(.pred) * (abs(.pred)^5))
```

```{r}
submission <- read_csv(file.path(file_path, "sampleSubmission.csv.zip"))
submission$Weekly_Sales <- result$.pred
write.csv(submission, row.names = FALSE,
          "ridge_regression_tuned.csv")
submission %>% head()
```


